# -*- coding: utf-8 -*-
"""Predictive Analysis - Credit Card Fraud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VJljv4mbmBJ5KobyX2ZUs8wGoIWjiXVd

# **Install Environment**
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d mlg-ulb/creditcardfraud

!mkdir credit-card-fraud
!unzip -qq creditcardfraud.zip -d credit-card-fraud

!ls credit-card-fraud/

"""# **Exploratory Data Analysis**"""

import pandas as pd

# Save dataset into variable
main_data = pd.read_csv('credit-card-fraud/creditcard.csv')

"""Variable Description"""

# Check Shape Data
shape = main_data.shape
print("Shape data:", shape)

# Variable Information
main_data.info()

# Statistical Description
main_data.describe()

"""Check Missing Value and Outliers"""

# Check Missing Value
main_data.isnull().sum()

import matplotlib.pyplot as plt

# Check Outliers
plt.figure(figsize=(8, 6))
main_data.boxplot()
plt.title('Boxplot untuk Dataset')
plt.xticks(rotation=90)
plt.show()

target_0 = main_data[main_data['Class'] == 0]
target_1 = main_data[main_data['Class'] == 1]

# Tampilkan data target 0
print("Data Target 0:")
print(target_0.head())

# Tampilkan data target 1
print("Data Target 1:")
print(target_1.head())

# # IQR
# Q1 = main_data.quantile(0.25)
# Q3 = main_data.quantile(0.75)
# IQR = Q3 - Q1

# # Outliers identification
# outliers = ((main_data < (Q1 - 1.5 * IQR)) | (main_data > (Q3 + 1.5 * IQR))).any(axis=1)

# print("Data with outliers:")
# print(main_data[outliers])

# # Handle outliers
# data_cleaned = main_data[~outliers]  # Delete rows with outliers
# # data_cleaned = df.mask(outliers, df.mean())  # If want to change outliers value with mean

# print("Data without outliers:")
# print(data_cleaned)

"""Univariate Analysis"""

import matplotlib.pyplot as plt

# Check Histogram
main_data.hist(bins=50, figsize=(20,15))
plt.show()

"""Multivariate Analysis"""

import seaborn as sns

# Correlation Matrix
corr = main_data.corr()
#plt.figure(figsize=(20,15))
sns.set(rc={'figure.figsize':(25,20)})
# plot a heatmap
sns.heatmap(corr, cbar = True, annot=True, fmt= '.2f',annot_kws={'size': 10},
           xticklabels= main_data.columns, yticklabels= main_data.columns,
           cmap= 'viridis')

"""# **Data Preparation**"""

# Find the variables with correlation greater than 0.5 or less than -0.5
high_correlation_vars = []
for col in corr:
    correlated_vars = corr[(corr[col] > 0.5) | (corr[col] < -0.5)].index.tolist()
    correlated_vars.remove(col)  # Exclude self-correlation
    high_correlation_vars.extend(correlated_vars)

# Remove duplicates from the list
high_correlation_vars = list(set(high_correlation_vars))
print(high_correlation_vars)

data1 = main_data.drop('V2', axis=1)

from sklearn.preprocessing import StandardScaler

# Separate Data Target
target = data1['Class']
features = data1.drop('Class', axis=1)

# Data Standardization
numerical_cols = features.select_dtypes(include='number').columns
scaler = StandardScaler()
features[numerical_cols] = scaler.fit_transform(features[numerical_cols])
data2 = pd.concat([features, target], axis=1)

print(data2.head())

data2.describe()

"""SMOTE"""

from sklearn.model_selection import train_test_split

X = data2.drop('Class', axis=1)
y = data2['Class']
X_trains, X_tests, y_trains, y_tests = train_test_split(X, y, test_size = 0.2)

print(f"Train Data: {X_trains.shape}, {y_trains.shape}")
print(f"Test Data: {X_tests.shape}, {y_tests.shape}")

from imblearn.over_sampling import SMOTE
from collections import Counter

upsample = SMOTE()
X_train, y_train = upsample.fit_resample(X_trains, y_trains)
counter = Counter(y_trains)
counter

class1 = data2.loc[(main_data["Class"]==1)]
class0 = data2.loc[(main_data["Class"]==0)]

class1_over = class1.sample(data2["Class"].value_counts()[0], replace=True)
test_over = pd.concat([class1_over, class0], axis=0)

test_over["Class"].value_counts()

data = test_over.copy().reset_index()
data.head()

"""Split Data"""

from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

# Split Data
X = data.drop('Class', axis=1)
y = data['Class']
X, y = shuffle(X, y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

print(f'Total of sample in whole dataset: {len(X)}')
print(f'Total of sample in train dataset: {len(X_train)}')
print(f'Total of sample in test dataset: {len(X_test)}')

"""# **Model Development**

K-Nearest Neighbors
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import confusion_matrix, accuracy_score

# K-Nearest Neighbors (KNN)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

train_accuracy_knn = knn.score(X_train, y_train)
test_accuracy_knn = accuracy_score(y_test, y_pred_knn)

print("K-Nearest Neighbors (KNN):")
print("Train Accuracy:", train_accuracy_knn)
print("Test Accuracy:", test_accuracy_knn)
print()
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_knn))

"""Random Forest"""

rf = RandomForestClassifier(random_state=123)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

train_accuracy_rf = rf.score(X_train, y_train)
test_accuracy_rf = accuracy_score(y_test, y_pred_rf)

print("Random Forest:")
print("Train Accuracy:", train_accuracy_rf)
print("Test Accuracy:", test_accuracy_rf)
print()
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))

"""Boosting Algorithm"""

gb = GradientBoostingClassifier(random_state=123)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)

train_accuracy_gb = gb.score(X_train, y_train)
test_accuracy_gb = accuracy_score(y_test, y_pred_gb)

print("Gradient Boosting:")
print("Train Accuracy:", train_accuracy_gb)
print("Test Accuracy:", test_accuracy_gb)
print()
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_gb))

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

mse_knn = mean_squared_error(y_test, y_pred_knn)
rmse_knn = mean_squared_error(y_test, y_pred_knn, squared=False)
r2_knn = r2_score(y_test, y_pred_knn)

mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)
r2_rf = r2_score(y_test, y_pred_rf)

mse_gb = mean_squared_error(y_test, y_pred_gb)
rmse_gb = mean_squared_error(y_test, y_pred_gb, squared=False)
r2_gb = r2_score(y_test, y_pred_gb)

print("K-Nearest Neighbors")
print("MSE:", mse_knn)
print("RMSE:", rmse_knn)
print("R2 Score:", r2_knn)
print()
print("Random Forest")
print("MSE:", mse_rf)
print("RMSE:", rmse_rf)
print("R2 Score:", r2_rf)
print()
print("Gradient Boosting")
print("MSE:", mse_gb)
print("RMSE:", rmse_gb)
print("R2 Score:", r2_gb)
print()